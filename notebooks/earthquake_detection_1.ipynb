{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde97174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3790e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/merge.hdf5\"\n",
    "\n",
    "\n",
    "hdf5_file = h5py.File(file_path, 'r')\n",
    "data_group = hdf5_file['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e073e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes in sample :  {'back_azimuth_deg': 159.3, 'coda_end_sample': array([[2896.]], dtype=float32), 'network_code': 'TA', 'p_arrival_sample': 700.0, 'p_status': 'manual', 'p_travel_sec': 17.079999923706055, 'p_weight': 0.5, 'receiver_code': '109C', 'receiver_elevation_m': 150.0, 'receiver_latitude': 32.8889, 'receiver_longitude': -117.1051, 'receiver_type': 'BH', 's_arrival_sample': 1894.0, 's_status': 'manual', 's_weight': 0.5, 'snr_db': array([56.79999924, 55.40000153, 47.40000153]), 'source_depth_km': 0.45, 'source_depth_uncertainty_km': 'None', 'source_distance_deg': 0.92, 'source_distance_km': 102.09, 'source_error_sec': 1.1119, 'source_gap_deg': 107.466, 'source_horizontal_uncertainty_km': 4.6403, 'source_id': '8556349', 'source_latitude': 33.7496, 'source_longitude': -117.4938, 'source_magnitude': 3.6, 'source_magnitude_author': 'None', 'source_magnitude_type': 'ml', 'source_mechanism_strike_dip_rake': 'None', 'source_origin_time': '2006-07-23 15:58:50.88', 'source_origin_uncertainty_sec': 0.47, 'trace_category': 'earthquake_local', 'trace_name': '109C.TA_20060723155859_EV', 'trace_start_time': '2006-07-23 15:59:00.960000'}\n",
      "Key :  109C.TA_20060723155859_EV\n"
     ]
    }
   ],
   "source": [
    "sample_key = list(data_group.keys())[0]\n",
    "sample = data_group[sample_key]\n",
    "\n",
    "print(\"Attributes in sample : \",dict(sample.attrs))\n",
    "print(\"Key : \",sample_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15f0bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: earthquake_local\n",
      "Reciever code :  109C\n"
     ]
    }
   ],
   "source": [
    "attrs = sample.attrs\n",
    "print(\"Label:\", attrs['trace_category'])\n",
    "print(\"Reciever code : \",attrs['receiver_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e2ba4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subset keys: 70000\n"
     ]
    }
   ],
   "source": [
    "earthquake_keys = np.load(\"../preprocessed/earthquake_keys.npy\", allow_pickle=True)\n",
    "noise_keys = np.load(\"../preprocessed/noise_keys.npy\", allow_pickle=True)\n",
    "subset_keys = np.concatenate([earthquake_keys, noise_keys])\n",
    "\n",
    "print(f\"Total subset keys: {len(subset_keys)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7891c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_label = {}\n",
    "station_to_keys = defaultdict(list)\n",
    "\n",
    "for key in subset_keys:\n",
    "    attrs = data_group[key].attrs\n",
    "    label = 1 if attrs[\"trace_category\"] == \"earthquake_local\" else 0\n",
    "    station = attrs[\"receiver_code\"]\n",
    "    key_to_label[key] = label\n",
    "    station_to_keys[station].append(key)\n",
    "\n",
    "print(f\"Total stations in subset: {len(station_to_keys)}\")\n",
    "\n",
    "with open(\"../preprocessed/key_to_label.pkl\", \"wb\") as f:\n",
    "    pickle.dump(key_to_label, f)\n",
    "\n",
    "with open(\"../preprocessed/station_to_keys.pkl\", \"wb\") as f:\n",
    "    pickle.dump(station_to_keys, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "282c22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../preprocessed/key_to_label.pkl\", \"rb\") as f:\n",
    "    key_to_label = pickle.load(f)\n",
    "\n",
    "with open(\"../preprocessed/station_to_keys.pkl\", \"rb\") as f:\n",
    "    station_to_keys = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd8a0619",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = list(station_to_keys.keys())\n",
    "random.shuffle(stations)\n",
    "\n",
    "n_total = len(stations)\n",
    "n_train = int(0.8 * n_total)\n",
    "n_val = int(0.1 * n_total)\n",
    "\n",
    "train_stations = stations[:n_train]\n",
    "val_stations = stations[n_train:n_train + n_val]\n",
    "test_stations = stations[n_train + n_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23642838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_samples(station_list):\n",
    "  return np.array([\n",
    "    (key, key_to_label[key])\n",
    "    for st in station_list\n",
    "    for key in station_to_keys[st]\n",
    "  ], dtype=object)\n",
    "\n",
    "train_samples = collect_samples(train_stations)\n",
    "val_samples   = collect_samples(val_stations)\n",
    "test_samples  = collect_samples(test_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ddd7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../preprocessed/train_split.npy\", train_samples)\n",
    "np.save(\"../preprocessed/val_split.npy\", val_samples)\n",
    "np.save(\"../preprocessed/test_split.npy\", test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b41141f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train processing: 100%|██████████| 57789/57789 [20:39<00:00, 46.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train saved: (57789, 65, 95, 3), (57789,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val processing: 100%|██████████| 7232/7232 [02:27<00:00, 48.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val saved: (7232, 65, 95, 3), (7232,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test processing: 100%|██████████| 4979/4979 [01:46<00:00, 46.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test saved: (4979, 65, 95, 3), (4979,)\n"
     ]
    }
   ],
   "source": [
    "from scipy.signal import stft\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "os.makedirs(\"../preprocessed/spectrograms\", exist_ok=True)\n",
    "\n",
    "# STFT parameters\n",
    "fs = 100  # Hz (STEAD sampling rate)\n",
    "nperseg = 128\n",
    "noverlap = 64\n",
    "nfft = 128\n",
    "\n",
    "def preprocess_sample(hdf5_file, key):\n",
    "    sample = hdf5_file['data'][key][:]\n",
    "    sample = (sample - sample.mean(axis=0)) / (sample.std(axis=0) + 1e-6)\n",
    "\n",
    "    specs = []\n",
    "    for ch in range(3):\n",
    "        f, t, Zxx = stft(sample[:, ch], fs=fs, nperseg=nperseg,\n",
    "                         noverlap=noverlap, nfft=nfft)\n",
    "        Sxx = np.abs(Zxx)\n",
    "        Sxx = (Sxx - Sxx.min()) / (Sxx.max() - Sxx.min() + 1e-6)\n",
    "        specs.append(Sxx)\n",
    "\n",
    "    return np.stack(specs, axis=-1).astype(np.float32)\n",
    "\n",
    "def preprocess_split(split_name):\n",
    "    samples = np.load(f\"../preprocessed/{split_name}_split.npy\", allow_pickle=True)\n",
    "    X_list, y_list = [], []\n",
    "    with h5py.File(file_path, \"r\") as f:\n",
    "        for i, (key, label) in enumerate(tqdm(samples, desc=f\"{split_name} processing\")):\n",
    "            spec = preprocess_sample(f, key)\n",
    "            X_list.append(spec)\n",
    "            y_list.append(label)\n",
    "\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    np.save(f\"../preprocessed/spectrograms/X_{split_name}.npy\", X)\n",
    "    np.save(f\"../preprocessed/spectrograms/y_{split_name}.npy\", y)\n",
    "    print(f\"{split_name} saved: {X.shape}, {y.shape}\")\n",
    "\n",
    "\n",
    "preprocess_split(\"train\")\n",
    "preprocess_split(\"val\")\n",
    "preprocess_split(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e6970b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6379eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"../preprocessed/spectrograms/X_train.npy\")\n",
    "y_train = np.load(\"../preprocessed/spectrograms/y_train.npy\")\n",
    "X_val = np.load(\"../preprocessed/spectrograms/X_val.npy\")\n",
    "y_val = np.load(\"../preprocessed/spectrograms/y_val.npy\")\n",
    "X_test = np.load(\"../preprocessed/spectrograms/X_test.npy\")\n",
    "y_test = np.load(\"../preprocessed/spectrograms/y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37a315c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (57789, 65, 95, 3), Val: (7232, 65, 95, 3), Test: (4979, 65, 95, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d434738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomTranslation(height_factor=0.03, width_factor=0.05),\n",
    "    layers.RandomZoom(0.05),\n",
    "    layers.RandomContrast(0.2),\n",
    "    layers.Lambda(lambda x: x + tf.random.normal(tf.shape(x), stddev=0.02))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9caea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "def make_dataset(X,y,training=False):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((X,y))\n",
    "  if training:\n",
    "    dataset = dataset.shuffle(buffer_size=2000)\n",
    "    dataset = dataset.map(lambda x,y : (data_augmentation(x,training=True),y),\n",
    "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "  dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7c822b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = make_dataset(X_train, y_train, training=True)\n",
    "val_ds = make_dataset(X_val, y_val)\n",
    "test_ds = make_dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2610a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X shape: (32, 65, 95, 3)\n",
      "Batch Y shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "for batch_x, batch_y in train_ds.take(1):\n",
    "  print(\"Batch X shape:\", batch_x.shape)\n",
    "  print(\"Batch Y shape:\", batch_y.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "577d19ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_7 (Conv2D)           (None, 65, 95, 16)        448       \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 65, 95, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 32, 47, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 32, 47, 32)        4640      \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 32, 47, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 16, 23, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 16, 23, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 16, 23, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " global_average_pooling2d_2   (None, 64)               0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,257\n",
      "Trainable params: 28,033\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_cnn(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "\n",
    "        layers.Conv2D(16, (3,3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_cnn(X_train.shape[1:])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cd52d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "008514e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1806/1806 [==============================] - 36s 19ms/step - loss: 0.1134 - accuracy: 0.9623 - val_loss: 0.3192 - val_accuracy: 0.8630\n",
      "Epoch 2/25\n",
      "1806/1806 [==============================] - 40s 22ms/step - loss: 0.0574 - accuracy: 0.9821 - val_loss: 0.7476 - val_accuracy: 0.7045\n",
      "Epoch 3/25\n",
      "1806/1806 [==============================] - 37s 20ms/step - loss: 0.0418 - accuracy: 0.9870 - val_loss: 1.1333 - val_accuracy: 0.6847\n",
      "Epoch 4/25\n",
      "1806/1806 [==============================] - 37s 20ms/step - loss: 0.0333 - accuracy: 0.9899 - val_loss: 0.6961 - val_accuracy: 0.8204\n",
      "Epoch 5/25\n",
      "1806/1806 [==============================] - 37s 20ms/step - loss: 0.0296 - accuracy: 0.9910 - val_loss: 0.1592 - val_accuracy: 0.9403\n",
      "Epoch 6/25\n",
      "1806/1806 [==============================] - 44s 24ms/step - loss: 0.0261 - accuracy: 0.9920 - val_loss: 0.1906 - val_accuracy: 0.9257\n",
      "Epoch 7/25\n",
      "1806/1806 [==============================] - 42s 23ms/step - loss: 0.0218 - accuracy: 0.9934 - val_loss: 0.0766 - val_accuracy: 0.9757\n",
      "Epoch 8/25\n",
      "1806/1806 [==============================] - 37s 21ms/step - loss: 0.0197 - accuracy: 0.9938 - val_loss: 0.0727 - val_accuracy: 0.9748\n",
      "Epoch 9/25\n",
      "1806/1806 [==============================] - 38s 21ms/step - loss: 0.0169 - accuracy: 0.9949 - val_loss: 0.0268 - val_accuracy: 0.9918\n",
      "Epoch 10/25\n",
      "1806/1806 [==============================] - 37s 20ms/step - loss: 0.0175 - accuracy: 0.9946 - val_loss: 0.0178 - val_accuracy: 0.9947\n",
      "Epoch 11/25\n",
      "1806/1806 [==============================] - 37s 20ms/step - loss: 0.0161 - accuracy: 0.9950 - val_loss: 0.0217 - val_accuracy: 0.9939\n",
      "Epoch 12/25\n",
      "1806/1806 [==============================] - 37s 20ms/step - loss: 0.0164 - accuracy: 0.9950 - val_loss: 0.2538 - val_accuracy: 0.9175\n",
      "Epoch 13/25\n",
      "1806/1806 [==============================] - 37s 21ms/step - loss: 0.0149 - accuracy: 0.9953 - val_loss: 0.0732 - val_accuracy: 0.9759\n",
      "Epoch 14/25\n",
      "1806/1806 [==============================] - 37s 21ms/step - loss: 0.0131 - accuracy: 0.9961 - val_loss: 0.0394 - val_accuracy: 0.9873\n",
      "Epoch 15/25\n",
      "1806/1806 [==============================] - 37s 21ms/step - loss: 0.0139 - accuracy: 0.9958 - val_loss: 0.0203 - val_accuracy: 0.9938\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=25,               \n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2dc1e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 1s 7ms/step - loss: 0.0103 - accuracy: 0.9966\n",
      "Test Loss: 0.0103\n",
      "Test Accuracy: 0.9966\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f590886d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 1s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Noise       0.99      1.00      1.00      2145\n",
      "  Earthquake       1.00      0.99      1.00      2834\n",
      "\n",
      "    accuracy                           1.00      4979\n",
      "   macro avg       1.00      1.00      1.00      4979\n",
      "weighted avg       1.00      1.00      1.00      4979\n",
      "\n",
      "Confusion Matrix:\n",
      " [[2144    1]\n",
      " [  16 2818]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "y_pred_probs = model.predict(test_ds)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "\n",
    "y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Noise\",\"Earthquake\"]))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c223df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf210",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
